{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "- Jorge Caballeros Pérez - 20009\n",
    "- Mario de León - 19019\n",
    "- Pablo Escobar - 20936 \n",
    "\n",
    "20/8/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "#### **1. ¿Qué es Prioritized sweeping para ambientes determinísticos?**\n",
    "Es un método en el que se priorizan actualizaciones en estados que más impactan la política en ambientes donde las acciones siempre producen el mismo resultado, lo que hace que el aprendizaje sea más rápido y eficiente.\n",
    "\n",
    "#### **2. ¿Qué es Trajectory Sampling?**\n",
    "Es un enfoque que toma muestras de trayectorias completas (secuencia de estados y acciones) para estimar el valor de una política o entrenar un modelo, en lugar de actualizar estado por estado.\n",
    "\n",
    "#### **3. ¿Qué es Upper Confidence Bounds para Árboles (UCT por sus siglas en inglés)?**\n",
    "Es un algoritmo que equilibra exploración y explotación en árboles de decisión, usando una fórmula que agrega un término de confianza para explorar ramas menos conocidas mientras sigue explotando las mejores opciones conocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Crear el entorno FrozenLake-v1\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "# Definir una función para simular la interacción con el entorno\n",
    "def simulate_action(env, state, action):\n",
    "    env.reset()\n",
    "    env.env.state = state\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Variables del entorno\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "    \n",
    "    def expand(self, action, next_state):\n",
    "        if action not in self.children:\n",
    "            self.children[action] = Node(next_state, parent=self)\n",
    "        return self.children[action]\n",
    "    \n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "        if self.parent:\n",
    "            self.parent.update(reward)\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb1(node, c_param=1.4):\n",
    "    # Calcula el valor UCB1 para un nodo\n",
    "    return node.value / (node.visits + 1e-6) + c_param * np.sqrt(np.log(node.parent.visits + 1) / (node.visits + 1e-6))\n",
    "\n",
    "def select_best_action(node, c_param=1.4):\n",
    "    return max(node.children.items(), key=lambda item: ucb1(item[1], c_param))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, node, max_depth=100):\n",
    "    current_node = node\n",
    "    total_reward = 0\n",
    "    depth = 0\n",
    "    \n",
    "    while depth < max_depth:\n",
    "        action = np.random.choice(action_space)\n",
    "        next_state, reward, done = simulate_action(env, current_node.state, action)\n",
    "        \n",
    "        if action not in current_node.children:\n",
    "            current_node = current_node.expand(action, next_state)\n",
    "        \n",
    "        total_reward += reward\n",
    "        depth += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(node):\n",
    "    action = np.random.choice(action_space)\n",
    "    next_state, reward, done = simulate_action(env, node.state, action)\n",
    "    \n",
    "    if action not in node.children:\n",
    "        return node.expand(action, next_state), reward, done\n",
    "    else:\n",
    "        return node, 0, False\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.value += reward\n",
    "        node = node.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor acción seleccionada es: Arriba \n"
     ]
    }
   ],
   "source": [
    "def mcts(env, root, n_simulations=1000):\n",
    "    for _ in range(n_simulations):\n",
    "        node = root\n",
    "        \n",
    "        # Fase de selección\n",
    "        while node.is_fully_expanded() and node.children:\n",
    "            action = select_best_action(node)\n",
    "            node = node.children[action]\n",
    "        \n",
    "        # Fase de expansión\n",
    "        node, reward, done = expand_node(node)\n",
    "        \n",
    "        # Fase de simulación solo si no es un estado terminal\n",
    "        if not done:\n",
    "            total_reward = simulate(env, node)\n",
    "            total_reward += reward  # Suma la recompensa inmediata\n",
    "        else:\n",
    "            total_reward = reward  # Si es terminal, solo usa la recompensa inmediata\n",
    "        \n",
    "        # Fase de retropropagación\n",
    "        backpropagate(node, total_reward)\n",
    "    \n",
    "    return select_best_action(root)\n",
    "\n",
    "\n",
    "# Crear la raíz del árbol en el estado inicial\n",
    "initial_state = env.reset()[0]\n",
    "root = Node(initial_state)\n",
    "\n",
    "# Ejecutar MCTS\n",
    "best_action = mcts(env, root)\n",
    "actions = ['Izquierda','Abajo','Derecha','Arriba'] \n",
    "\n",
    "\n",
    "print(f\"La mejor acción seleccionada es: {actions[best_action]} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
